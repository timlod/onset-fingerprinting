#+TITLE: Modelling lags
#+AUTHOR: Tim Loderhose
#+EMAIL: tim@loderhose.com
#+DATE: Wednesday, 12 June 2024
#+STARTUP: showall
#+PROPERTY: header-args :exports both :session lags :kernel lm :cache no
:PROPERTIES:
OPTIONS: ^:nil
#+LATEX_COMPILER: xelatex
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [logo, color, author]
#+LATEX_HEADER: \insertauthor
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage[left=0.75in,top=0.6in,right=0.75in,bottom=0.6in]{geometry}
:END:

* Imports and Environment Variables
:PROPERTIES:
:visibility: folded
:END:

#+name: imports
#+begin_src python
import json
from importlib import reload
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import sounddevice as sd
import soundfile as sf
import torch
import torch.nn as nn
from onset_fingerprinting import (
    calibration,
    detection,
    model,
    multilateration,
    plots,
)
from torch import optim
from torch.nn import functional as F
#+end_src

#+name: env
#+begin_src python
device = "cuda"
#+end_src

* Introduction
I would like to move towards using NNs on windows of time-correlated audio. For
this, I need to be able to find a network architecture which can learn these
temporal relationships between signals. If the network can figure out the lag
between the different channels, then it should be able to learn the physical
model relating those to the location as well.

To this end, I will generate some impulse data for which I know the lags, and
plug in a number of architectures to see which one can learn this challenge.

* Generate data

The simplest version of this problem finds the lags between impulses occuring
in two signals of a length w.
#+begin_src python
def generate_data(w: int, c: int, n: int = 10000, device=None):
    signals = torch.zeros(n, c, w, device=device)
    impulses = torch.randint(0, w, (n, c), device=device)
    # Force some 0s
    impulses[0] = torch.tensor([0] * c)
    impulses[1] = torch.tensor([w // 2] * c)
    impulses[2] = torch.tensor([w - 1] * c)
    # Force the extremes
    z = torch.zeros((c, c), dtype=torch.long)
    for i in range(c):
        z[i, i] = w - 1
    impulses[3 : 3 + c] = z
    signals.scatter_(2, impulses[:, :, None], 1)
    return signals, torch.diff(impulses).to(torch.float32), impulses
#+end_src


* Learning

#+begin_src python
def train(
    x,
    y,
    model,
    lossfun,
    optimizer,
    scheduler,
    num_epochs=3000,
    x_val=None,
    y_val=None,
    patience=None,
    max_norm: float = 1.0,
    print_every: int = 100,
    print_examples: bool = True,
    device=None,
):
    model.to(device)
    x.to(device)
    y.to(device)
    errors = []
    last_loss = torch.inf
    best_model = None
    counter = 0
    for epoch in range(num_epochs):
        optimizer.zero_grad(set_to_none=True)
        pred = model(x)
        error = lossfun(pred, y)
        loss = error.mean()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)
        optimizer.step()
        scheduler.step()
        if x_val is not None:
            with torch.no_grad():
                vp = model(x_val)
                ve = lossfun(vp, y_val)
            errors.append((error.item(), ve.item()))
            if patience is not None:
                if ve < last_loss:
                    last_loss = ve
                    best_model = model
                    counter = 0
                elif counter < patience:
                    counter += 1
                else:
                    print(f"Stopping at epoch {epoch}!")
                    break
        else:
            errors.append(error.item())
        if epoch % print_every == 0:
            print(
                f"Epoch {epoch}, TL"
                f" {loss.item()} {f'VL: {ve.item()}' if x_val is not None else ''}"
            )
    print(f"Epoch {epoch}, Loss {loss.item()}")
    if print_examples:
        print(pred[:10], "\n", y[:10])
    return errors, best_model


def error_analysis(model, tx, ty, timp, n_samp=100):
    tp = model.cpu()(tx.cpu())
    e = F.l1_loss(tp, ty.cpu(), reduction="none").squeeze()
    print(
        f"Mean loss: {e.mean().item():4f}, Median loss:"
        f" {e.median().item():.4f}"
    )
    fig = plt.figure(figsize=(6, 3))
    fig.suptitle(f"First {n_samp} test samples")
    plt.plot(tp[:n_samp].detach().cpu(), label="Predictions")
    plt.plot(ty[:n_samp].cpu(), label="Truth")
    plt.legend()
    if e.ndim == 2:
        e = e.mean(1)
    sortidx = e.argsort()
    fig = plt.figure(figsize=(6, 3))
    ax = fig.add_subplot(111)
    (a,) = ax.plot(e[sortidx].detach(), label="Sorted test errors")
    ax.set_ylabel("Errors")
    (b,) = ax.twinx().plot(
        ty.max(1).values.abs().cpu()[sortidx],
        label="Max lag in prediction",
        color="tab:orange",
        alpha=0.7,
    )
    lines = [a, b]
    labels = [line.get_label() for line in lines]
    plt.legend(lines, labels)
    print(
        "Best:",
        ty.cpu()[sortidx][:20, 0],
        "\nWorst:",
        ty.cpu()[sortidx][-20:, 0],
    )
    print(timp[sortidx][-20:])
#+end_src

** 2 channels
Let's start with the simplest version:
: torch.Size([100, 1, 256, 16])
#+begin_src python
w = 256
c = 2
lossfun = F.mse_loss
lr = 0.002 * (10 if lossfun == F.mse_loss else 1)
num_epochs = 2000
print_every = 100

m = model.CNN(w, c - 1, c, layer_sizes=[8, 16, 32, 16, 8], kernel_size=3).to(
    device
)
#m = model.RNN(w, c - 1, c, 16, 2, dropout_rate=0.6, rnn_type="GRU", share_input_weights=False).to(device)
# m = model.CNNRNN(
#     w,
#     c - 1,
#     c,
#     layer_sizes=[64],
#     kernel_size=7,
#     n_hidden=16,
#     n_rnn_layers=2,
#     dropout_rate=0.6,
# ).to(device)
x, y, imp = generate_data(w, c, 100, device=device)
tx, ty, timp = generate_data(w, c, 1000, device=device)

optimizer = optim.NAdam(m.parameters(), lr=lr, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 2000)

errors, bm = train(
    x, y, m, lossfun, optimizer, scheduler, 3000, tx[:100], ty[:100], 500
)
#+end_src

#+RESULTS:
#+begin_example
Epoch 0, TL 11923.2119140625 VL: 12653.0439453125
Epoch 100, TL 1250.1591796875 VL: 5643.154296875
Epoch 200, TL 1180.04248046875 VL: 1848.591796875
Epoch 300, TL 402.5008850097656 VL: 1662.59619140625
Epoch 400, TL 459.6202392578125 VL: 677.0028076171875
Epoch 500, TL 296.0764465332031 VL: 779.1265258789062
Epoch 600, TL 307.20916748046875 VL: 510.5166320800781
Epoch 700, TL 285.1921081542969 VL: 574.2542724609375
Epoch 800, TL 324.3214111328125 VL: 477.7702331542969
Epoch 900, TL 214.35459899902344 VL: 506.7640380859375
Epoch 1000, TL 365.494140625 VL: 576.2450561523438
Epoch 1100, TL 319.63641357421875 VL: 414.01129150390625
Epoch 1200, TL 215.15426635742188 VL: 430.5967102050781
Epoch 1300, TL 324.72705078125 VL: 537.046630859375
Epoch 1400, TL 250.90687561035156 VL: 434.5767517089844
Epoch 1500, TL 209.4241485595703 VL: 627.3638916015625
Epoch 1600, TL 233.42921447753906 VL: 457.96502685546875
Epoch 1700, TL 213.8964385986328 VL: 499.7232360839844
Epoch 1800, TL 214.24624633789062 VL: 429.812255859375
Epoch 1900, TL 246.33677673339844 VL: 444.5519714355469
Epoch 2000, TL 217.6715545654297 VL: 416.34075927734375
Epoch 2100, TL 243.9999542236328 VL: 416.56866455078125
Stopping at epoch 2165!
Epoch 2165, Loss 210.11903381347656
tensor([[   1.0917],
        [   1.7738],
        [  -0.6566],
        [-272.9344],
        [ 212.0352],
        [ -17.7385],
        [ 189.7748],
        [ -76.0503],
        [ 142.1835],
        [  32.7605]], device='cuda:0', grad_fn=<SliceBackward0>) 
 tensor([[   0.],
        [   0.],
        [   0.],
        [-255.],
        [ 255.],
        [ -27.],
        [ 169.],
        [ -94.],
        [ 148.],
        [  30.]], device='cuda:0')
#+end_example

#+begin_src python :async no
error_analysis(bm, tx, ty, timp)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Mean loss: 2.188316, Median loss: 1.0464
Best: tensor([ -23., -178.,   41.,  -38.,  -63., -205.,  -42.,  -25.,   31.,  -82.,
        -129., -132., -102.,  -59.,  -92.,  -63., -172.,   41.,  -30.,  -15.]) 
Worst: tensor([188.,   2., 204.,  -4., 178., 167.,   0., 221.,   2., 175., 170.,   2.,
          3.,   2., 245., 246.,   2., -58.,  -3.,  -1.])
tensor([[ 58, 246],
        [ 84,  86],
        [ 48, 252],
        [ 35,  31],
        [ 75, 253],
        [  1, 168],
        [ 23,  23],
        [  1, 222],
        [ 99, 101],
        [  1, 176],
        [  1, 171],
        [131, 133],
        [ 11,  14],
        [ 42,  44],
        [  4, 249],
        [  5, 251],
        [ 69,  71],
        [ 58,   0],
        [ 22,  19],
        [104, 103]], device='cuda:0')
#+end_example
[[./.ob-jupyter/d9964bdb83eaa449b0b62fc837f874893dbf7f47.png]]
[[./.ob-jupyter/0aedf47a76e3b5335b919aa39b76c889116de172.png]]
:END:

Although it doesn't always converge, this works! Both RNN and CNN are able to
do this, in fact.

However, the loss on the full test set is still rather high! It looks like it's
primarily very large or very small/nonexisting lags which cause this issue.
Large lags make sense, as they're at the boundary and thus are closer to
require extrapolation.

Notes RNN:
- I needed to have a hidden size of 128+ to be able to learn this properly, at
  2 layers. More layers, and it becomes harder to learn. With smaller sizes, it
  appears that the lag is limited to the hidden size, showing that it is
  related to how far the network can look to find lags.
- Once I added the attention, it worked also with a hidden size of 64
Notes CNN:
- slightly worse at this than the RNN in convergence - it gets better at larger
  numbers of parameters, but then I'd need to tweak more to get it to converge

** 3 channels
Let's see if it can learn 2 lags at the same time. That's one step closer
towards what we need to learn.

#+begin_src python
w = 256
c = 3
lossfun = F.mse_loss
lr = 0.001 * (10 if lossfun == F.mse_loss else 1)
num_epochs = 3000
print_every = 100

# m = model.CNN(
#     w, c-1, c, layer_sizes=[8, 16, 32, 16, 8], kernel_size=3, dilation=1
# ).cuda()
m = model.RNN(w, c - 1, c, 16, 2, dropout_rate=0.6, share_input_weights=True).cuda()
device = m.device
x, y, imp = generate_data(w, c, 100, device=device)
tx, ty, timp = generate_data(w, c, 1000, device=device)

optimizer = optim.NAdam(m.parameters(), lr=lr, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)

errors, bm = train(
    x, y, m, lossfun, optimizer, scheduler, 3000, tx[:100], ty[:100], 500
)
#+end_src

#+RESULTS:
#+begin_example
Epoch 0, TL 9797.724609375 VL: 10937.0771484375
Epoch 100, TL 4533.7890625 VL: 9037.7470703125
Epoch 200, TL 2950.878173828125 VL: 3322.52294921875
Epoch 300, TL 356.5895080566406 VL: 1074.657470703125
Epoch 400, TL 202.80323791503906 VL: 867.0380859375
Epoch 500, TL 84.18399047851562 VL: 386.94635009765625
Epoch 600, TL 32.605918884277344 VL: 74.24748229980469
Epoch 700, TL 64.31904602050781 VL: 41.493675231933594
Epoch 800, TL 17.088197708129883 VL: 57.716644287109375
Epoch 900, TL 28.063058853149414 VL: 31.811717987060547
Epoch 1000, TL 13.49834156036377 VL: 28.634328842163086
Epoch 1100, TL 15.28337574005127 VL: 26.30498504638672
Epoch 1200, TL 11.294228553771973 VL: 26.616729736328125
Epoch 1300, TL 10.797918319702148 VL: 16.320541381835938
Epoch 1400, TL 7.7080979347229 VL: 15.624723434448242
Epoch 1500, TL 9.873404502868652 VL: 11.996635437011719
Epoch 1600, TL 5.244534969329834 VL: 13.392248153686523
Epoch 1700, TL 4.024059772491455 VL: 9.139055252075195
Epoch 1800, TL 4.523504257202148 VL: 13.074235916137695
Epoch 1900, TL 4.394941806793213 VL: 9.586922645568848
Epoch 2000, TL 4.473787307739258 VL: 9.338714599609375
Epoch 2100, TL 3.0711374282836914 VL: 10.5660400390625
Epoch 2200, TL 3.194096088409424 VL: 7.816829681396484
Epoch 2300, TL 2.5959553718566895 VL: 6.540218353271484
Epoch 2400, TL 2.9732067584991455 VL: 7.469451904296875
Epoch 2500, TL 2.7218360900878906 VL: 7.070628643035889
Epoch 2600, TL 2.3775062561035156 VL: 7.469150066375732
Epoch 2700, TL 2.0485284328460693 VL: 8.085326194763184
Epoch 2800, TL 2.4743940830230713 VL: 9.536794662475586
Epoch 2900, TL 2.2067036628723145 VL: 7.413653373718262
Epoch 2999, Loss 2.2365968227386475
tensor([[-1.8919e+00, -6.3282e-02],
        [ 1.1337e+00, -9.9705e-01],
        [-8.5596e-02, -1.4792e-01],
        [-2.5889e+02,  2.2042e-01],
        [ 2.5255e+02, -2.5264e+02],
        [ 6.9334e-01,  2.5559e+02],
        [-8.9901e+00,  1.6475e+02],
        [ 3.1594e+01, -5.2260e+01],
        [ 5.1377e+01,  3.8554e+01],
        [ 2.3470e+02, -2.6096e+01]], device='cuda:0', grad_fn=<SliceBackward0>) 
 tensor([[   0.,    0.],
        [   0.,    0.],
        [   0.,    0.],
        [-255.,    0.],
        [ 255., -255.],
        [   0.,  255.],
        [  -7.,  164.],
        [  32.,  -53.],
        [  51.,   38.],
        [ 236.,  -28.]], device='cuda:0')
#+end_example

Plot results on the test set:
#+begin_src python :async no
error_analysis(bm, tx, ty, timp)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Mean loss: 3.067169, Median loss: 1.9193
Best: tensor([-132.,   95.,    8.,   56.,    8., -208.,  -38.,   -2.,   16.,   43.,
          29., -164.,  -18.,  193.,   14.,   21.,  -48.,    6., -104.,  -59.]) 
Worst: tensor([ 37., 190., 255.,   5.,   0.,   4.,   5.,  -6., -60., 121., -26.,  31.,
        -81.,  73.,  71.,  59.,  43., 119.,   7.,  76.])
tensor([[  9,  46,  31],
        [ 18, 208, 254],
        [  0, 255,   0],
        [ 33,  38, 255],
        [246, 246, 191],
        [215, 219, 151],
        [220, 225, 170],
        [252, 246, 237],
        [163, 103, 254],
        [133, 254, 178],
        [146, 120, 255],
        [222, 253, 254],
        [237, 156, 255],
        [182, 255, 114],
        [184, 255, 242],
        [138, 197, 255],
        [204, 247, 253],
        [ 71, 190, 255],
        [248, 255,  94],
        [165, 241, 255]], device='cuda:0')
#+end_example
[[./.ob-jupyter/f68729cc5000a20cf33bed2d4bf8fb5f0a6d8c10.png]]
[[./.ob-jupyter/9b6d49aadc3afb461346e114ee8a746b8efd775b.png]]
:END:



#+RESULTS:
#+begin_example
Epoch 0, TL 12356.1396484375 VL: 11654.6298828125
Epoch 100, TL 7621.8505859375 VL: 8922.38671875
Epoch 200, TL 4692.791015625 VL: 4432.2724609375
Epoch 300, TL 3617.2890625 VL: 3960.30810546875
Epoch 400, TL 2975.39501953125 VL: 3386.6318359375
Epoch 500, TL 1673.6810302734375 VL: 1782.166015625
Epoch 600, TL 601.9627075195312 VL: 1070.9232177734375
Epoch 700, TL 438.4246826171875 VL: 714.2683715820312
Epoch 800, TL 252.6402130126953 VL: 660.646240234375
Epoch 900, TL 208.8948211669922 VL: 413.8019714355469
Epoch 1000, TL 163.1772918701172 VL: 311.8202819824219
Epoch 1100, TL 128.8693389892578 VL: 320.7862548828125
Epoch 1200, TL 112.0771255493164 VL: 292.47454833984375
Epoch 1300, TL 64.62334442138672 VL: 387.5838317871094
Epoch 1400, TL 86.0174560546875 VL: 215.64512634277344
Epoch 1500, TL 78.3893051147461 VL: 212.8132781982422
Epoch 1600, TL 58.031585693359375 VL: 217.86044311523438
Epoch 1700, TL 39.056209564208984 VL: 220.63156127929688
Epoch 1800, TL 32.34804916381836 VL: 189.09466552734375
Epoch 1900, TL 24.82532501220703 VL: 196.97238159179688
Epoch 2000, TL 24.550607681274414 VL: 175.1767120361328
Epoch 2100, TL 24.274049758911133 VL: 187.39707946777344
Epoch 2200, TL 15.048283576965332 VL: 170.42678833007812
Epoch 2300, TL 14.50401782989502 VL: 155.8015594482422
Epoch 2400, TL 14.956853866577148 VL: 164.1660919189453
Epoch 2500, TL 13.131484985351562 VL: 160.4081573486328
Epoch 2600, TL 11.323251724243164 VL: 155.822998046875
Epoch 2700, TL 11.416837692260742 VL: 158.9982147216797
Epoch 2800, TL 13.83969497680664 VL: 150.00393676757812
Epoch 2900, TL 9.069437980651855 VL: 163.32676696777344
Epoch 2999, Loss 11.212181091308594
tensor([[  -0.2927,   -3.0392],
        [  -0.7276,   -2.8642],
        [   0.6274,    0.3188],
        [-252.9344,   -1.5700],
        [ 256.4276, -252.8566],
        [   5.4303,  249.7804],
        [ 129.4544,  -98.4681],
        [-132.0554,   46.2599],
        [ -14.3961,   92.2857],
        [ -64.0335,    7.8503]], device='cuda:0', grad_fn=<SliceBackward0>) 
 tensor([[   0.,    0.],
        [   0.,    0.],
        [   0.,    0.],
        [-255.,    0.],
        [ 255., -255.],
        [   0.,  255.],
        [ 126.,  -96.],
        [-135.,   46.],
        [ -12.,   82.],
        [ -67.,    9.]], device='cuda:0')
#+end_example

#+begin_example
Mean loss: 9.929891, Median loss: 5.3763
Best: tensor([-236.,   86.,    0.,   -8.,   27.,  163.,   71.,  229.,   50.,  126.,
          82.,  -45.,  163.,  128.,    5.,   30.,  -27., -116.,   49.,  176.]) 
Worst: tensor([-178.,  123.,  159.,  162.,  152.,  184.,  195.,  176.,  158.,  178.,
         160.,  150.,  175.,  184.,  182.,  184.,  210.,  201.,  197.,  205.])
tensor([[202,  24,  45],
        [ 91, 214,  98],
        [ 86, 245,  90],
        [ 15, 177,  25],
        [ 51, 203,  55],
        [  7, 191,   6],
        [  5, 200,   2],
        [ 64, 240,  43],
        [ 37, 195,  22],
        [ 10, 188,  16],
        [ 87, 247, 106],
        [ 60, 210,  67],
        [ 40, 215,  29],
        [ 22, 206,  18],
        [ 61, 243,  79],
        [ 58, 242,  44],
        [ 16, 226,   3],
        [ 15, 216,  23],
        [ 51, 248,  65],
        [ 43, 248,  27]], device='cuda:0')
#+end_example
[[./.ob-jupyter/758e66ff4cd77bc94a894c5f05d9ba3ddd4ef35c.png]]
[[./.ob-jupyter/7fcba408499a538dea4778611957b1d615e06577.png]]

Error analysis:
The MSE is still very high on this, possibly because we overfit, having lowered
the dropout.
let's see at which values of lags the model struggles most:
#+begin_src python
e = (tp - ty.cpu()).square().sum(1)
sortidx = e.argsort()
print("Best:\n",ty.cpu()[sortidx][:10].T, "\nWorst:\n", ty.cpu()[sortidx][-10:].T)
#+end_src

#+RESULTS:
: Best:
:  tensor([[ -55., -136.,  -55.,  119., -185.,   88., -182.,  206.,  104., -106.],
:         [ 105.,  115.,  -46., -141.,   88., -140.,  122., -101., -169.,   58.]]) 
: Worst:
:  tensor([[ 254.,  244.,  246.,    5.,  -89.,  240.,   29.,  -76., -187.,  -45.],
:         [ -76.,  -31.,  -53.,    0.,  166.,  -16.,  158.,  201.,  251.,  233.]])

There are somewhat more extreme values at the large errors, but in general I
think it's just overfit.

** Non-binary impulses
This is a contrived case where we learn impulses, but in reality we'll never
have such data. Let's transform these into gaussian impulses for a further
step, and check whether it still works as well.

#+begin_src python
def transform_impulse1(x, n=11, ramp_up: int = 0):
    c = x.shape[1]
    ls = torch.linspace(-3 * np.e, 0, n, device=x.device)
    exp = torch.exp(ls)
    if ramp_up > 0:
        exp[-ramp_up:] = torch.exp(
            torch.linspace(ls[-ramp_up], 2 * -np.e, ramp_up, device=x.device)
        )
    return F.conv1d(F.pad(x, (n - 1, 0)), exp.repeat(c, 1, 1), groups=c)
#+end_src

#+begin_src python
w = 256
c = 3
lossfun = F.l1_loss
lr = 0.001 * (10 if lossfun == F.mse_loss else 1)
num_epochs = 3000
print_every = 100

# m = model.CNN(
#     w, c-1, c, layer_sizes=[8, 16, 32, 16, 8], kernel_size=3, dilation=1
# ).to(device)
# m = model.CNNRNN(
#     w,
#     c-1,
#     c,
#     layer_sizes=[8],
#     kernel_size=2,
#     n_hidden=128,
#     n_rnn_layers=1,
#     dropout_rate=0.6,
# ).to(device)
# m = model.RNN(w, c - 1, c, 64, 2, dropout_rate=0.5).to(device)
m = model.LCCCNN(
    w,
    c-1,
    c,
    layer_sizes=[8, 8, 8, 8],
    kernel_sizes=7,
    dropout_rate=0.0,
    batch_norm=True,
    loss=lossfun,
    lr=lr,
).to(device)
x, y, imp = generate_data(w, c, 100, device=device)
x = transform_impulse1(x, 200, 20)
tx, ty, timp = generate_data(w, c, 1000, device=device)
tx = transform_impulse1(tx, 200, 20)
y /= 255
ty /= 255
optimizer = optim.NAdam(m.parameters(), lr=lr, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)

errors, bm = train(
    x, y, m, lossfun, optimizer, scheduler, 5000, tx[:100], ty[:100], 500
)
#+end_src

#+RESULTS:
#+begin_example
/home/tim/projects/onset-fingerprinting/venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
Epoch 0, TL 11471.6171875 VL: 10763.947265625
Epoch 100, TL 6300.701171875 VL: 4763.0048828125
Epoch 200, TL 1386.6453857421875 VL: 1241.863037109375
Epoch 300, TL 342.417724609375 VL: 723.0066528320312
Epoch 400, TL 250.0272216796875 VL: 510.8902893066406
Epoch 500, TL 55.00023651123047 VL: 379.0447998046875
Epoch 600, TL 57.27154541015625 VL: 424.559326171875
Epoch 700, TL 43.54613494873047 VL: 150.87136840820312
Epoch 800, TL 31.350616455078125 VL: 146.08096313476562
Epoch 900, TL 37.63465881347656 VL: 210.27972412109375
Epoch 1000, TL 59.4703254699707 VL: 61.12222671508789
Epoch 1100, TL 29.809720993041992 VL: 64.12410736083984
Epoch 1200, TL 15.877347946166992 VL: 97.62782287597656
Epoch 1300, TL 14.474164962768555 VL: 87.64909362792969
Epoch 1400, TL 13.176837921142578 VL: 86.94642639160156
Epoch 1500, TL 7.699976444244385 VL: 81.4412841796875
Epoch 1600, TL 5.240980625152588 VL: 65.48567199707031
Epoch 1700, TL 9.369585037231445 VL: 61.48301315307617
Epoch 1800, TL 11.597272872924805 VL: 55.46167755126953
Epoch 1900, TL 11.893485069274902 VL: 46.76387405395508
Epoch 2000, TL 5.205259323120117 VL: 56.14391326904297
Epoch 2100, TL 6.685842037200928 VL: 60.57660675048828
Epoch 2200, TL 2.979496955871582 VL: 47.39455795288086
Epoch 2300, TL 2.6737499237060547 VL: 52.31924819946289
Epoch 2400, TL 2.32865309715271 VL: 56.26995849609375
Epoch 2500, TL 2.1595070362091064 VL: 60.19451904296875
Epoch 2600, TL 2.235826015472412 VL: 53.08357238769531
Stopping at epoch 2637!
Epoch 2637, Loss 2.036637306213379
tensor([[  62.6798,  141.3915],
        [  68.4154, -164.8867],
        [  53.9853,   59.8275],
        [  31.9432, -179.9884],
        [ 173.9814,   21.4797],
        [  46.0552,  -49.5891],
        [ 123.8873, -126.6256],
        [ -19.2127,   14.2151],
        [-150.5007,  182.4794],
        [ -93.4469, -122.0795]], device='cuda:0', grad_fn=<SliceBackward0>) 
 tensor([[  58.,  143.],
        [  65., -163.],
        [  55.,   59.],
        [  32., -180.],
        [ 175.,   20.],
        [  49.,  -52.],
        [ 123., -126.],
        [ -15.,   12.],
        [-149.,  181.],
        [ -93., -122.]], device='cuda:0')
#+end_example

#+begin_src python :async no
error_analysis(bm, tx, ty, timp)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Mean loss: 3.942275, Median loss: 1.8895
Best: tensor([ 69.,  35.,  56., -40.,  64.,  52.,  47., 129., -76., 151.,  79.,  -5.,
         55.,  40., 132., -50., -20.,  53., -41.,  31.]) 
Worst: tensor([ -62., -122.,   -1., -226.,   -1., -201., -209.,  -70., -223.,  -86.,
        -229.,  -74.,    3.,  -82.,    3.,  -90.,   -2.,  -89.,    0.,    2.])
tensor([[ 56, 118,  88],
        [ 18, 140, 115],
        [ 69,  70, 173],
        [ 18, 244, 174],
        [104, 105, 189],
        [  8, 209, 191],
        [ 11, 220, 173],
        [ 67, 137, 118],
        [ 25, 248, 224],
        [ 10,  96,  79],
        [ 22, 251, 196],
        [ 33, 107,  73],
        [121, 118, 158],
        [ 21, 103,  81],
        [ 59,  56, 151],
        [  8,  98,  69],
        [ 71,  73,  68],
        [  3,  92,  69],
        [ 42,  42, 199],
        [ 29,  27, 110]], device='cuda:0')
#+end_example
[[./.ob-jupyter/a8aa1a861fad67c9f96828b56d97206fc25181dc.png]]
[[./.ob-jupyter/7d0c1568492492549fec03849e787f041c31e2d2.png]]
:END:

Nice, it performs pretty much the same!

*** Additional changes
This is still very idealized - here are more things we can do to make it look
more real:
- peaks at different amplitudes
- modulate with sine wave
- add noise


Note: frequencies should be the same in each of the channels, phase could be
slightly shifted, but very little. The sine needs to start at the impulse in
each case, so currently this is wrong.
#+begin_src python
def transform_impulse2(
    x, imp, random_phase: bool = False, noise_std=0, sr=96000
):
    n, c, w = x.shape
    ls = torch.linspace(0, x.shape[-1] / sr, x.shape[-1], device=x.device)
    phase = (
        torch.rand(x.shape[0], x.shape[1], 1, device=x.device) * 0.1 * np.pi
        if random_phase
        else 0
    )
    f = torch.randint(300, 1000, (x.shape[0], 1, 1), device=x.device).expand(
        n, c, 1
    )
    sin = torch.sin(2 * np.pi * ls[None, None, :] * f + phase)
    for i in range(len(x)):
        for j in range(c):
            k = w - imp[i, j]
            x[i, j, imp[i, j] :] *= sin[i, j, :k]
    x += torch.randn(x.shape, device=x.device) * noise_std
    return x
#+end_src

#+begin_src python
x = transform_impulse2(x, imp, True, 0.001)
tx = transform_impulse2(tx, timp, True, 0.001)

optimizer = optim.NAdam(m.parameters(), lr=lr, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)

errors, bm = train(
    x,
    y,
    m.to(device),
    lossfun,
    optimizer,
    scheduler,
    3000,
    tx[:100],
    ty[:100],
    500,
)
#+end_src

#+RESULTS:
#+begin_example
Epoch 0, TL 38.41012191772461 VL: 11656.9599609375
Epoch 100, TL 16.59128189086914 VL: 153.88595581054688
Epoch 200, TL 22.921138763427734 VL: 109.77005004882812
Epoch 300, TL 60.49607467651367 VL: 96.41871643066406
Epoch 400, TL 11399.8642578125 VL: 10716.013671875
Epoch 500, TL 3034.529296875 VL: 3443.92236328125
Epoch 600, TL 689.79443359375 VL: 455.908203125
Epoch 700, TL 72.95342254638672 VL: 165.76028442382812
Stopping at epoch 732!
Epoch 732, Loss 80.4583511352539
tensor([[  61.4558,  144.0829],
        [  48.9084, -145.4959],
        [  49.2660,   55.4530],
        [  22.9640, -167.7405],
        [ 166.9901,   23.7873],
        [  45.7351,  -44.0062],
        [ 110.5431, -113.2652],
        [ -17.7397,    7.5596],
        [-141.3242,  174.2570],
        [-105.1589, -120.3260]], device='cuda:0', grad_fn=<SliceBackward0>) 
 tensor([[  58.,  143.],
        [  65., -163.],
        [  55.,   59.],
        [  32., -180.],
        [ 175.,   20.],
        [  49.,  -52.],
        [ 123., -126.],
        [ -15.,   12.],
        [-149.,  181.],
        [ -93., -122.]], device='cuda:0')
#+end_example

#+begin_src python :async no
error_analysis(bm, tx, ty, timp)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Mean loss: 8.697659, Median loss: 7.2153
Best: tensor([  81., -133.,  -21., -149.,   37.,  -50.,   44.,  -51., -194.,   36.,
         -82., -117.,   -8., -165.,  142.,  133.,  160.,   47.,  -70.,  -55.]) 
Worst: tensor([153., -16.,  13., -25., -11., -15., -20., -24., -18., -32., -24.,  -6.,
        -14.,  11., -21.,  11., -18., 228., 196.,  -2.])
tensor([[186,  33,   2],
        [  9,  25,  26],
        [141, 128, 118],
        [ 29,  54, 162],
        [  8,  19,  37],
        [ 59,  74, 160],
        [ 13,  33, 162],
        [ 13,  37, 124],
        [ 38,  56, 175],
        [ 38,  70,  60],
        [ 19,  43, 106],
        [ 24,  30,  84],
        [ 45,  59, 125],
        [ 69,  58,  46],
        [ 30,  51, 129],
        [ 87,  76,  70],
        [  2,  20,  62],
        [254,  26,  17],
        [208,  12,   8],
        [ 71,  73,  68]], device='cuda:0')
#+end_example
[[./.ob-jupyter/8c75740c54df5a4ef929e0528f428af74278893a.png]]
[[./.ob-jupyter/471d27d84a130b38e16a2cb2baf00a9975c3c161.png]]
:END:


Good sizes appear to be either a few (5) 4-8-size layers with a large kernel size
(e.g. 33) or 10 layers with a moderate kernel size (e.g. 15).
#+begin_src python
m = model.CNN(
    w, c - 1, c, layer_sizes=[8, 16, 32, 16, 8], kernel_size=3, dilation=1
).to(device)
m = model.CNNRNN(
    w,
    c-1,
    c,
    layer_sizes=[8],
    kernel_size=2,
    n_hidden=128,
    n_rnn_layers=1,
    dropout_rate=0.6,
).to(device)
m = model.LCCCNN(
    w,
    c-1,
    c,
    layer_sizes=[5] * 7,
    kernel_sizes=[1, 33, 64, 15, 15, 15, 1],
    dropout_rate=0.0,
    batch_norm=True,
    loss=lossfun,
    lr=lr,
    group=False
).to(device)
#m = model.RNN(w, c - 1, c, 64, 2, dropout_rate=0.5).to(device)

x, y, imp = generate_data(w, c, 100, device=device)
x = transform_impulse1(x, 200, 20)
x = transform_impulse2(x, imp, True, 0.001)
tx, ty, timp = generate_data(w, c, 1000, device=device)
tx = transform_impulse1(tx, 200, 20)
tx = transform_impulse2(tx, timp, True, 0.001)
y /= 255
ty /= 255
optimizer = optim.NAdam(m.parameters(), lr=lr, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)

errors, bm = train(
    x, y, m, lossfun, optimizer, scheduler, 3000, tx[:100], ty[:100], 500
)
#+end_src

#+RESULTS:
#+begin_example
/home/tim/projects/onset-fingerprinting/venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
Epoch 0, TL 0.34366485476493835 VL: 0.3255743384361267
Epoch 100, TL 0.26445454359054565 VL: 0.2550080120563507
Epoch 200, TL 0.16149179637432098 VL: 0.16676898300647736
Epoch 300, TL 0.09406105428934097 VL: 0.09764253348112106
Epoch 400, TL 0.04544714838266373 VL: 0.06738411635160446
Epoch 500, TL 0.03269654139876366 VL: 0.0470535084605217
Epoch 600, TL 0.022350525483489037 VL: 0.04292641580104828
Epoch 700, TL 0.020343920215964317 VL: 0.03690901771187782
Epoch 800, TL 0.020186061039566994 VL: 0.0329962857067585
Epoch 900, TL 0.02146865241229534 VL: 0.03518415614962578
Epoch 1000, TL 0.014311742037534714 VL: 0.032402947545051575
Epoch 1100, TL 0.01714521460235119 VL: 0.03262735903263092
Epoch 1200, TL 0.016986042261123657 VL: 0.03241237625479698
Epoch 1300, TL 0.010854646563529968 VL: 0.02708239108324051
Epoch 1400, TL 0.01329617016017437 VL: 0.028283804655075073
Epoch 1500, TL 0.01340155303478241 VL: 0.029462534934282303
Epoch 1600, TL 0.009539321064949036 VL: 0.03070862963795662
Epoch 1700, TL 0.008093439973890781 VL: 0.026886966079473495
Epoch 1800, TL 0.007949438877403736 VL: 0.026835067197680473
Epoch 1900, TL 0.00765299191698432 VL: 0.02683980017900467
Epoch 2000, TL 0.006239107809960842 VL: 0.02680799923837185
Epoch 2100, TL 0.006872530560940504 VL: 0.025939149782061577
Epoch 2200, TL 0.005762790329754353 VL: 0.025423364713788033
Epoch 2300, TL 0.005310623440891504 VL: 0.024932218715548515
Epoch 2400, TL 0.005013849586248398 VL: 0.0253605879843235
Epoch 2500, TL 0.004759583156555891 VL: 0.025143718346953392
Epoch 2600, TL 0.004527405370026827 VL: 0.025112859904766083
Epoch 2700, TL 0.004365041386336088 VL: 0.02509929984807968
Stopping at epoch 2795!
Epoch 2795, Loss 0.0042651318944990635
tensor([[ 1.2110e-02, -7.4396e-03],
        [-1.7612e-04,  1.0390e-02],
        [-5.9767e-05, -3.0183e-03],
        [-9.5502e-01,  1.7124e-03],
        [ 9.8836e-01, -9.7110e-01],
        [-7.9518e-05,  9.4503e-01],
        [ 2.3557e-01,  4.4383e-01],
        [-4.6320e-01,  6.3517e-01],
        [ 6.5157e-01, -6.8630e-01],
        [-1.0973e-01,  6.4423e-01]], device='cuda:0', grad_fn=<SliceBackward0>) 
 tensor([[ 0.0000,  0.0000],
        [ 0.0000,  0.0000],
        [ 0.0000,  0.0000],
        [-1.0000,  0.0000],
        [ 1.0000, -1.0000],
        [ 0.0000,  1.0000],
        [ 0.2353,  0.4431],
        [-0.4627,  0.6353],
        [ 0.6431, -0.6863],
        [-0.1098,  0.6431]], device='cuda:0')
#+end_example



#+begin_src python :async no
error_analysis(bm, tx, ty, timp)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Mean loss: 0.051738, Median loss: 0.0324
Best: tensor([ 0.1059,  0.1686, -0.6157, -0.2627,  0.1882,  0.6039,  0.5961,  0.0196,
        -0.5059,  0.6078, -0.3333, -0.5333,  0.0745, -0.6627, -0.3647,  0.4863,
         0.2118, -0.7451, -0.3098, -0.7569]) 
Worst: tensor([0.3490, 0.4784, 0.4157, 0.4588, 0.5686, 0.2784, 0.4118, 0.5686, 0.4353,
        0.5020, 0.5294, 0.5059, 0.3843, 0.4941, 0.2902, 0.4196, 0.2745, 0.3961,
        0.2941, 0.3490])
tensor([[114, 203, 167],
        [ 35, 157,  21],
        [133, 239, 230],
        [ 98, 215, 151],
        [ 79, 224, 128],
        [175, 246, 199],
        [150, 255, 214],
        [ 83, 228, 127],
        [110, 221, 160],
        [124, 252, 190],
        [ 99, 234, 173],
        [116, 245, 151],
        [102, 200, 134],
        [113, 239, 186],
        [139, 213, 182],
        [138, 245, 188],
        [135, 205, 159],
        [114, 215, 129],
        [166, 241, 184],
        [164, 253, 206]], device='cuda:0')
#+end_example
[[file:./.ob-jupyter/8dd0e3f72555fafb6e9aada4736d071235470ace.png]]
[[file:./.ob-jupyter/b697d9a14a82cd517e7a50738cd23bb2233ae5a0.png]]
:END:


*** Making the data even more real

In its current iteration, the data models an impulse of the fundamental - but
as far as the modelling problem goes, it's different from what we'll see in
realtime: There, we'll always start the window from the first onset on. In the
current data, the first onset may start anywhere.

Let's adapt the data in such a way that our first onset is always close to the
beginning of the buffers.
#+begin_src python
def generate_data2(w: int, c: int, n: int = 10000, max_shift=10, device=None):
    signals = torch.zeros(n, c, w, device=device)
    impulses = torch.randint(0, w - max_shift, (n, c), device=device)
    mini = impulses.min(dim=1, keepdim=True).values
    impulses -= mini
    impulses += torch.maximum(
        torch.tensor(0, device=device),
        torch.minimum(
            w - impulses.max(dim=1, keepdim=True).values - 1,
            torch.randint(max_shift, (len(impulses), 1), device=device),
        ),
    )
    # Force some 0s
    impulses[0] = torch.tensor([0] * c)
    impulses[1] = torch.tensor([w // 2] * c)
    impulses[2] = torch.tensor([w - 1] * c)
    # Force the extremes
    z = torch.zeros((c, c), dtype=torch.long)
    for i in range(c):
        z[i, i] = w - 1
    impulses[3 : 3 + c] = z
    signals.scatter_(2, impulses[:, :, None], 1)
    return signals, torch.diff(impulses).to(torch.float32), impulses
#+end_src


#+begin_src python
w = 256
c = 3
lossfun = F.mse_loss
lr = 0.001 * (5 if lossfun == F.mse_loss else 1)
num_epochs = 2000
print_every = 100

m = model.CNN(
    w, c - 1, c, layer_sizes=[8, 8], kernel_size=8, dropout_rate=0.9
).to(device)
m = model.RNN(
    w,
    c - 1,
    c,
    16,
    1,
    dropout_rate=0.6,
    rnn_type="GRU",
    share_input_weights=True,
).to(device)
# m = model.CNNRNN(
#     w,
#     c - 1,
#     c,
#     layer_sizes=[9, 18, 27],
#     kernel_size=3,
#     n_hidden=64,
#     n_rnn_layers=2,
#     dropout_rate=0.8,
#     groups=1,
# ).to(device)
m = model.LCCCNN(
    w,
    c-1,
    c,
    layer_sizes=[5] * 7,
    kernel_sizes=[1, 33, 64, 15, 15, 15, 1],
    dropout_rate=0.0,
    batch_norm=True,
    loss=lossfun,
    lr=lr,
    group=False
).to(device)
x, y, imp = generate_data2(w, c, 1000, 100, device=device)
tx, ty, timp = generate_data2(w, c, 1000, 100, device=device)
y /= 255
ty /= 255

optimizer = optim.NAdam(m.parameters(), lr=lr, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, num_epochs / 10
)
errors, bm = train(
    x, y, m, lossfun, optimizer, scheduler, 3000, tx[:100], ty[:100], 500
)
x = transform_impulse1(x, 200, 20)
tx = transform_impulse1(tx, 200, 20)
errors, bm = train(
    x, y, m, lossfun, optimizer, scheduler, 3000, tx[:100], ty[:100], 500
)
x = transform_impulse2(x, imp, True, 0.01)
tx = transform_impulse2(tx, timp, True, 0.01)
errors, bm = train(
    x, y, m, lossfun, optimizer, scheduler, 3000, tx[:100], ty[:100], 500
)
#+end_src

#+RESULTS:
#+begin_example
Epoch 0, TL 4395.76611328125 VL: 5299.62255859375
Epoch 100, TL 3365.845703125 VL: 4822.96044921875
Epoch 200, TL 2772.103271484375 VL: 3702.37548828125
Epoch 300, TL 3322.334228515625 VL: 4290.53662109375
Epoch 400, TL 3812.298828125 VL: 3554.98876953125
Epoch 500, TL 2912.2646484375 VL: 2528.25927734375
Epoch 600, TL 1182.7371826171875 VL: 1790.157470703125
Epoch 700, TL 1227.01611328125 VL: 1872.30615234375
Epoch 800, TL 1848.1175537109375 VL: 1960.5078125
Epoch 900, TL 970.9756469726562 VL: 1661.15478515625
Epoch 1000, TL 518.984619140625 VL: 992.01708984375
Epoch 1100, TL 809.7820434570312 VL: 1747.26806640625
Epoch 1200, TL 2681.91357421875 VL: 2803.46337890625
Epoch 1300, TL 477.33197021484375 VL: 802.52294921875
Epoch 1400, TL 267.9732666015625 VL: 652.127197265625
Epoch 1500, TL 286.6626892089844 VL: 884.8186645507812
Epoch 1600, TL 854.4407958984375 VL: 855.2654418945312
Epoch 1700, TL 231.8495635986328 VL: 721.853271484375
Epoch 1800, TL 144.84832763671875 VL: 460.5245361328125
Epoch 1900, TL 169.9160919189453 VL: 735.3161010742188
Epoch 2000, TL 366.41845703125 VL: 978.0071411132812
Epoch 2100, TL 146.53395080566406 VL: 478.1092224121094
Epoch 2200, TL 105.03839874267578 VL: 330.9482421875
Epoch 2300, TL 115.1164779663086 VL: 526.8109741210938
Epoch 2400, TL 149.66419982910156 VL: 749.67724609375
Epoch 2500, TL 174.15863037109375 VL: 391.32769775390625
Epoch 2600, TL 74.22200775146484 VL: 268.6869812011719
Epoch 2700, TL 72.20063018798828 VL: 414.69439697265625
Epoch 2800, TL 97.6591796875 VL: 598.396484375
Epoch 2900, TL 68.07989501953125 VL: 290.2934875488281
Epoch 2999, Loss 51.260860443115234
tensor([[  10.1366,  -17.9054],
        [  13.1774,  -13.3129],
        [  12.9529,  -13.0401],
        [-173.3324,   17.5262],
        [ 157.5206, -155.4530],
        [ -22.7140,  177.0232],
        [  70.3581,  -46.2313],
        [  44.1151, -105.5530],
        [ 122.4708,  -89.5229],
        [  31.2111,  -75.7051]], device='cuda:0', grad_fn=<SliceBackward0>) 
 tensor([[   0.,    0.],
        [   0.,    0.],
        [   0.,    0.],
        [-255.,    0.],
        [ 255., -255.],
        [   0.,  255.],
        [  80.,  -49.],
        [  40., -104.],
        [ 128.,  -94.],
        [  33.,  -80.]], device='cuda:0')
Epoch 0, TL 3757.65576171875 VL: 3640.633056640625
Epoch 100, TL 247.66635131835938 VL: 409.28961181640625
Epoch 200, TL 211.8089141845703 VL: 291.97784423828125
Epoch 300, TL 51.52690124511719 VL: 170.4066162109375
Epoch 400, TL 20.654159545898438 VL: 99.1829605102539
Epoch 500, TL 39.93817138671875 VL: 88.76754760742188
Epoch 600, TL 74.1521224975586 VL: 166.62974548339844
Epoch 700, TL 30.708261489868164 VL: 65.65766906738281
Epoch 800, TL 8.916840553283691 VL: 43.72679901123047
Epoch 900, TL 31.219749450683594 VL: 31.562416076660156
Epoch 1000, TL 10.731572151184082 VL: 60.79820251464844
Epoch 1100, TL 8.798493385314941 VL: 25.034931182861328
Epoch 1200, TL 4.425315856933594 VL: 14.026777267456055
Epoch 1300, TL 5.048941135406494 VL: 13.20430850982666
Epoch 1400, TL 17.053970336914062 VL: 29.951086044311523
Epoch 1500, TL 5.24343729019165 VL: 13.691798210144043
Epoch 1600, TL 3.1189658641815186 VL: 9.197158813476562
Epoch 1700, TL 6.30896520614624 VL: 7.787571907043457
Epoch 1800, TL 15.738643646240234 VL: 19.0738582611084
Epoch 1900, TL 7.203464031219482 VL: 7.392098903656006
Epoch 2000, TL 2.2224698066711426 VL: 5.900872707366943
Epoch 2100, TL 3.703397035598755 VL: 4.962874412536621
Epoch 2200, TL 9.485461235046387 VL: 16.46314239501953
Epoch 2300, TL 2.431313991546631 VL: 6.579531192779541
Epoch 2400, TL 1.9502060413360596 VL: 4.203733921051025
Epoch 2500, TL 2.643656015396118 VL: 6.293785095214844
Epoch 2600, TL 4.755863666534424 VL: 22.464324951171875
Epoch 2700, TL 4.248551845550537 VL: 4.106677532196045
Epoch 2800, TL 1.663733720779419 VL: 2.814873695373535
Epoch 2900, TL 2.257847309112549 VL: 3.483154773712158
Epoch 2999, Loss 9.530533790588379
tensor([[  -2.8574,    1.1075],
        [   0.8431,    0.9241],
        [  -5.6148,   -1.9003],
        [-253.5990,   -1.0545],
        [ 251.6201, -247.6473],
        [  -7.9750,  246.4797],
        [  82.5954,  -50.0517],
        [  36.5455, -101.0688],
        [ 131.9486,  -92.9606],
        [  34.7109,  -80.0628]], device='cuda:0', grad_fn=<SliceBackward0>) 
 tensor([[   0.,    0.],
        [   0.,    0.],
        [   0.,    0.],
        [-255.,    0.],
        [ 255., -255.],
        [   0.,  255.],
        [  80.,  -49.],
        [  40., -104.],
        [ 128.,  -94.],
        [  33.,  -80.]], device='cuda:0')
Epoch 0, TL 106.94245910644531 VL: 99.25122833251953
Epoch 100, TL 10.359472274780273 VL: 21.093120574951172
Epoch 200, TL 6.979597091674805 VL: 17.06334114074707
Epoch 300, TL 6.004084587097168 VL: 15.146120071411133
Epoch 400, TL 12.52808666229248 VL: 16.590269088745117
Epoch 500, TL 6.083004951477051 VL: 11.449847221374512
Epoch 600, TL 4.390475273132324 VL: 10.803228378295898
Epoch 700, TL 4.0955634117126465 VL: 12.54408073425293
Epoch 800, TL 5.846721172332764 VL: 23.0767822265625
Epoch 900, TL 6.763631820678711 VL: 7.676110744476318
Epoch 1000, TL 3.189301013946533 VL: 5.618921279907227
Epoch 1100, TL 5.026641845703125 VL: 5.327602386474609
Epoch 1200, TL 6.647042274475098 VL: 7.531866073608398
Epoch 1300, TL 4.106977462768555 VL: 5.394028663635254
Epoch 1400, TL 2.40857195854187 VL: 4.836437702178955
Epoch 1500, TL 3.81868052482605 VL: 3.960578441619873
Epoch 1600, TL 4.547883987426758 VL: 9.753561973571777
Epoch 1700, TL 3.478538751602173 VL: 4.64895486831665
Epoch 1800, TL 1.8218472003936768 VL: 4.345370769500732
Epoch 1900, TL 2.1438331604003906 VL: 5.599161148071289
Epoch 2000, TL 3.403980255126953 VL: 10.333402633666992
Epoch 2100, TL 3.355443000793457 VL: 3.6203765869140625
Epoch 2200, TL 1.5254161357879639 VL: 2.8843836784362793
Epoch 2300, TL 2.344771146774292 VL: 2.935321569442749
Epoch 2400, TL 2.844273567199707 VL: 10.029642105102539
Epoch 2500, TL 1.3621636629104614 VL: 3.4204516410827637
Epoch 2600, TL 1.212609887123108 VL: 1.9826327562332153
Epoch 2700, TL 2.069546937942505 VL: 2.834421396255493
Epoch 2800, TL 5.372961521148682 VL: 3.4183897972106934
Epoch 2900, TL 1.7085667848587036 VL: 2.390336513519287
Epoch 2999, Loss 1.0286846160888672
tensor([[-5.6807e-01, -4.9543e-01],
        [ 5.8342e-01,  9.4363e-01],
        [-2.1604e-01,  4.6893e-01],
        [-2.5363e+02,  4.0343e-02],
        [ 2.5675e+02, -2.5548e+02],
        [-2.4753e+00,  2.5302e+02],
        [ 8.1115e+01, -4.9502e+01],
        [ 4.0290e+01, -1.0284e+02],
        [ 1.2700e+02, -9.2850e+01],
        [ 3.2659e+01, -7.8642e+01]], device='cuda:0', grad_fn=<SliceBackward0>) 
 tensor([[   0.,    0.],
        [   0.,    0.],
        [   0.,    0.],
        [-255.,    0.],
        [ 255., -255.],
        [   0.,  255.],
        [  80.,  -49.],
        [  40., -104.],
        [ 128.,  -94.],
        [  33.,  -80.]], device='cuda:0')
#+end_example

#+begin_src python :async no
error_analysis(bm, tx, ty, timp)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Mean loss: 0.007647, Median loss: 0.0059
Best: tensor([-0.1137,  0.3020, -0.2588, -0.0863, -0.1725, -0.1608,  0.1490, -0.4353,
        -0.4039, -0.3020, -0.0353, -0.3451,  0.1412,  0.1843, -0.2118, -0.0392,
         0.1412, -0.0039, -0.2118,  0.1922]) 
Worst: tensor([ 0.4000,  0.2078,  0.3961,  0.1255,  0.1843,  0.3059,  0.3020,  0.2706,
        -0.3333,  0.3843,  0.4706,  0.0431,  0.2471,  0.4745,  0.3961, -0.5804,
        -0.6039,  0.0000, -1.0000,  1.0000])
tensor([[ 97, 199, 138],
        [ 66, 119, 102],
        [ 25, 126, 108],
        [  1,  33,   0],
        [107, 154,   6],
        [ 70, 148, 104],
        [ 71, 148,  25],
        [ 87, 156, 215],
        [ 90,   5,  65],
        [103, 201,  79],
        [ 27, 147,  92],
        [ 98, 109, 216],
        [ 81, 144, 163],
        [ 99, 220, 102],
        [ 99, 200,  72],
        [238,  90, 228],
        [249,  95, 117],
        [  0,   0, 255],
        [255,   0,   0],
        [  0, 255,   0]], device='cuda:0')
#+end_example
[[file:./.ob-jupyter/16aa948859cc6fb7f431d3f989ae775eb6c81026.png]]
[[file:./.ob-jupyter/664d6dc41b2fc53e5caaf08382f290c9004cba8e.png]]
:END:

** Real Data


* Pre-training
Start with impulse data, and epoch-by-epoch morph it into something looking
more like a real signal.



* Idea
Random tone generator based on FM synthesis or just adding different modulated
sines with a huge space. Then feedback the system by saying like/dislike on
single tones to find a space of settings which are pleasing to the ear.

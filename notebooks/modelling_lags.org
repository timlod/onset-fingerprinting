#+TITLE: Modelling lags
#+AUTHOR: Tim Loderhose
#+EMAIL: tim@loderhose.com
#+DATE: Wednesday, 12 June 2024
#+STARTUP: showall
#+PROPERTY: header-args :exports both :session lags :kernel lm :cache no
:PROPERTIES:
OPTIONS: ^:nil
#+LATEX_COMPILER: xelatex
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [logo, color, author]
#+LATEX_HEADER: \insertauthor
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage[left=0.75in,top=0.6in,right=0.75in,bottom=0.6in]{geometry}
:END:

* Imports and Environment Variables
:PROPERTIES:
:visibility: folded
:END:

#+name: imports
#+begin_src python
import json
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import sounddevice as sd
import soundfile as sf
import torch
import torch.nn as nn
from onset_fingerprinting import (
    calibration,
    detection,
    model,
    multilateration,
    plots,
)
from torch import optim
from torch.nn import functional as F
#+end_src

#+name: env
#+begin_src python
device = "cpu"
#+end_src

* Introduction
I would like to move towards using NNs on windows of time-correlated audio. For
this, I need to be able to find a network architecture which can learn these
temporal relationships between signals. If the network can figure out the lag
between the different channels, then it should be able to learn the physical
model relating those to the location as well.

To this end, I will generate some impulse data for which I know the lags, and
plug in a number of architectures to see which one can learn this challenge.

* Generate data

The simplest version of this problem finds the lags between impulses occuring
in two signals of a length w.
#+begin_src python
def generate_data(w: int, c: int, n: int = 10000, device=None):
    signals = torch.zeros(n, c, w, device=device)
    impulses = torch.randint(0, w, (n, c), device=device)
    signals.scatter_(2, impulses[:, :, None], 1)
    lags = []
    for i in range(c - 1):
        lags.append(impulses[:, i] - impulses[:, i + 1])
    return signals, torch.stack(lags, -1).to(torch.float32), impulses
#+end_src


* Learning

#+begin_src python
def train(
    x,
    y,
    model,
    lossfun,
    optimizer,
    scheduler,
    num_epochs=3000,
    x_val=None,
    y_val=None,
    patience=None,
    max_norm: float = 1.0,
    print_every: int = 100,
    print_examples: bool = True,
    device=None,
):
    model.to(device)
    x.to(device)
    y.to(device)
    errors = []
    last_loss = torch.inf
    best_model = None
    counter = 0
    for epoch in range(num_epochs):
        optimizer.zero_grad(set_to_none=True)
        pred = model(x)
        error = lossfun(pred, y)
        loss = error.mean()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)
        optimizer.step()
        scheduler.step()
        if x_val is not None:
            with torch.no_grad():
                vp = model(x_val)
                ve = lossfun(vp, y_val)
            errors.append((error.item(), ve.item()))
            if patience is not None:
                if ve < last_loss:
                    last_loss = ve
                    best_model = model
                    counter = 0
                elif counter < patience:
                    counter += 1
                else:
                    print(f"Stopping at epoch {epoch}!")
                    break
        else:
            errors.append(error.item())
        if epoch % print_every == 0:
            print(
                f"Epoch {epoch}, TL"
                f" {loss.item()} {f'VL: {ve.item()}' if x_val is not None else ''}"
            )
    print(f"Epoch {epoch}, Loss {loss.item()}")
    if print_examples:
        print(pred[:10], "\n", y[:10])
    return errors, best_model

def error_analysis(model, tx, ty, timp, n_samp=100):
    tp = model.cpu()(tx.cpu())
    e = F.l1_loss(tp, ty.cpu(), reduction="none").squeeze()
    print(
        f"Mean loss: {e.mean().item():4f}, Median loss:"
        f" {e.median().item():.4f}"
    )
    fig = plt.figure(figsize=(6, 3))
    fig.suptitle(f"First {n_samp} test samples")
    plt.plot(tp[:n_samp].detach().cpu(), label="Predictions")
    plt.plot(ty[:n_samp].cpu(), label="Truth")
    plt.legend()
    if e.ndim == 2:
        e = e.mean(1)
    sortidx = e.argsort()
    plt.figure(figsize=(6, 3))
    plt.plot(e[sortidx].detach(), label="Sorted test errors")
    plt.legend()
    print(
        "Best:",
        ty.cpu()[sortidx][:20, 0],
        "\nWorst:",
        ty.cpu()[sortidx][-20:, 0],
    )
    print(timp[sortidx][-20:])
#+end_src

** 2 channels
Let's start with the simplest version:
#+begin_src python
w = 256
c = 2
lossfun = F.mse_loss
lr = 0.001 * (10 if lossfun == F.mse_loss else 1)
num_epochs = 2000
print_every = 100

m = model.CNN(w, 1, 2, layer_sizes=[8, 16, 32, 16, 8], kernel_size=3).to(
    device
)
# m = model.RNN(w, 1, 2, 64, 2, dropout_rate=0.6, rnn_type="GRU").to(device)
m = model.CNNRNN(
    w,
    1,
    2,
    layer_sizes=[32, 32, 64, 8],
    kernel_size=8,
    n_hidden=64,
    n_rnn_layers=2,
    dropout_rate=0.4,
).to(device)
x, y, imp = generate_data(w, c, 100, device=device)
tx, ty, timp = generate_data(w, c, 1000, device=device)

optimizer = optim.NAdam(m.parameters(), lr=lr, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 2000)

errors, bm = train(
    x, y, m, lossfun, optimizer, scheduler, 3000, tx[:100], ty[:100], 500
)
#+end_src

#+RESULTS:
#+begin_example
Epoch 0, TL 13353.39453125 VL: 11207.1923828125
Epoch 100, TL 882.7362670898438 VL: 3548.823486328125
Epoch 200, TL 401.40093994140625 VL: 1276.66259765625
Epoch 300, TL 20630.8984375 VL: 2484.170166015625
Epoch 400, TL 194.62771606445312 VL: 957.0549926757812
Epoch 500, TL 226.7541961669922 VL: 912.1659545898438
Epoch 600, TL 175.46380615234375 VL: 761.9036865234375
Epoch 700, TL 171.9024200439453 VL: 658.864501953125
Epoch 800, TL 124.95346069335938 VL: 597.2490234375
Epoch 900, TL 124.7151870727539 VL: 814.0574951171875
Epoch 1000, TL 132.40663146972656 VL: 781.1972045898438
Epoch 1100, TL 133.68292236328125 VL: 783.4812622070312
Epoch 1200, TL 73.37845611572266 VL: 992.8904418945312
Epoch 1300, TL 109.93061828613281 VL: 580.8544311523438
Epoch 1400, TL 132.23275756835938 VL: 452.6700134277344
Epoch 1500, TL 94.8537368774414 VL: 379.6468505859375
Epoch 1600, TL 90.6557388305664 VL: 714.870361328125
Epoch 1700, TL 68.4521484375 VL: 507.40264892578125
Epoch 1800, TL 80.26807403564453 VL: 512.697998046875
Epoch 1900, TL 84.19671630859375 VL: 463.88555908203125
Stopping at epoch 1998!
Epoch 1998, Loss 90.8318862915039
tensor([[ 102.4355],
        [-162.3792],
        [ -71.8653],
        [ -21.2326],
        [-138.5155],
        [ 131.6218],
        [  78.1376],
        [ 149.0656],
        [ -67.5249],
        [ -38.1569]], grad_fn=<SliceBackward0>) 
 tensor([[ 101.],
        [-153.],
        [ -66.],
        [ -22.],
        [-137.],
        [ 136.],
        [  74.],
        [ 144.],
        [ -70.],
        [ -43.]])
#+end_example

#+begin_src python :async no
error_analysis(bm, tx, ty, timp)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Mean loss: 2.577049, Median loss: 1.9602
Best: tensor([  60., -110., -105., -187.,   71., -105.,  -18.,   35.,   12.,  131.,
          -7.,  -29., -158.,  198.,  -17.,  -52.,  -69.,  130.,   20.,  -92.]) 
Worst: tensor([-228.,  112., -179.,  239.,   -1.,   87., -146.,   94.,   -9.,    1.,
        -247., -189.,    0., -112.,  -66.,    7., -169., -147., -144.,    0.])
tensor([[ 10, 238],
        [233, 121],
        [ 65, 244],
        [240,   1],
        [ 50,  51],
        [254, 167],
        [  0, 146],
        [255, 161],
        [  3,  12],
        [196, 195],
        [  4, 251],
        [  1, 190],
        [125, 125],
        [142, 254],
        [189, 255],
        [  9,   2],
        [ 86, 255],
        [108, 255],
        [111, 255],
        [ 17,  17]], device='cuda:0')
#+end_example
[[./.ob-jupyter/930be4c4af0001ac6ec6d76dd85ae984f9faad4c.png]]
[[./.ob-jupyter/8ef9fd4bf5c0c8a740ee0e711d9cf671c41eca90.png]]
:END:

Although it doesn't always converge, this works! Both RNN and CNN are able to
do this, in fact.

However, the loss on the full test set is still rather high! It looks like it's
primarily very large or very small/nonexisting lags which cause this issue.
Large lags make sense, as they're at the boundary and thus are closer to
require extrapolation.

Notes RNN:
- I needed to have a hidden size of 128+ to be able to learn this properly, at
  2 layers. More layers, and it becomes harder to learn. With smaller sizes, it
  appears that the lag is limited to the hidden size, showing that it is
  related to how far the network can look to find lags.
- Once I added the attention, it worked also with a hidden size of 64
Notes CNN:
- slightly worse at this than the RNN in convergence - it gets better at larger
  numbers of parameters, but then I'd need to tweak more to get it to converge

** 3 channels
Let's see if it can learn 2 lags at the same time. That's one step closer
towards what we need to learn.

#+begin_src python
w = 256
c = 3
lossfun = F.mse_loss
lr = 0.001 * (10 if lossfun == F.mse_loss else 1)
num_epochs = 3000
print_every = 100

# m = model.CNN(
#     w, c-1, c, layer_sizes=[8, 16, 32, 16, 8], kernel_size=3, dilation=1
# ).cuda()
m = model.RNN(w, c - 1, c, 64, 2, dropout_rate=0.5).cuda()
device = m.device
x, y, imp = generate_data(w, c, 100, device=device)
tx, ty, timp = generate_data(w, c, 1000, device=device)

optimizer = optim.NAdam(m.parameters(), lr=lr, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)

errors, bm = train(
    x, y, m, lossfun, optimizer, scheduler, 3000, tx[:100], ty[:100], 500
)
#+end_src

#+RESULTS:
#+begin_example
Epoch 0, TL 12731.3984375 VL: 11450.4462890625
Epoch 100, TL 7968.9736328125 VL: 5940.49462890625
Epoch 200, TL 2955.5166015625 VL: 3239.81396484375
Epoch 300, TL 523.3021240234375 VL: 717.0317993164062
Epoch 400, TL 320.8381042480469 VL: 512.6001586914062
Epoch 500, TL 235.69398498535156 VL: 360.5601806640625
Epoch 600, TL 109.55850982666016 VL: 343.000732421875
Epoch 700, TL 112.4301528930664 VL: 334.3623046875
Epoch 800, TL 167.77789306640625 VL: 273.8305358886719
Epoch 900, TL 88.43345642089844 VL: 302.03759765625
Epoch 1000, TL 73.77388763427734 VL: 319.9427795410156
Epoch 1100, TL 82.66132354736328 VL: 283.0507507324219
Epoch 1200, TL 49.167938232421875 VL: 321.13287353515625
Epoch 1300, TL 43.59089660644531 VL: 278.8407897949219
Epoch 1400, TL 42.160369873046875 VL: 286.0478515625
Epoch 1500, TL 37.47077560424805 VL: 291.0827941894531
Epoch 1600, TL 32.65715408325195 VL: 238.62306213378906
Epoch 1700, TL 16.179521560668945 VL: 253.12425231933594
Epoch 1800, TL 20.501623153686523 VL: 236.45606994628906
Epoch 1900, TL 13.568408966064453 VL: 254.19973754882812
Epoch 2000, TL 13.372800827026367 VL: 264.9991760253906
Epoch 2100, TL 10.062007904052734 VL: 239.95846557617188
Epoch 2200, TL 8.7745943069458 VL: 259.2027282714844
Epoch 2300, TL 6.862649917602539 VL: 230.3787384033203
Epoch 2400, TL 4.154443264007568 VL: 233.53456115722656
Epoch 2500, TL 3.5878753662109375 VL: 221.15550231933594
Epoch 2600, TL 3.6540887355804443 VL: 203.56600952148438
Epoch 2700, TL 4.884169101715088 VL: 201.5652618408203
Epoch 2800, TL 3.5546417236328125 VL: 208.3582763671875
Epoch 2900, TL 3.1885643005371094 VL: 207.4295654296875
Epoch 2999, Loss 3.4985568523406982
tensor([[-170.7369,  106.4263],
        [-116.4316,   18.8932],
        [-165.0470,   89.5156],
        [ -85.0047,  105.9716],
        [-213.6311,  223.8503],
        [  81.7890,  -13.6403],
        [ -30.4246,   85.0289],
        [   0.4911,  215.9208],
        [  59.3867, -224.3267],
        [ 112.0415,   -1.3033]], device='cuda:0', grad_fn=<SliceBackward0>) 
 tensor([[-168.,  105.],
        [-115.,   19.],
        [-165.,   89.],
        [ -85.,  107.],
        [-211.,  222.],
        [  81.,  -13.],
        [ -31.,   85.],
        [   4.,  214.],
        [  58., -222.],
        [ 109.,   -1.]], device='cuda:0')
#+end_example

Plot results on the test set:
#+begin_src python :async no
error_analysis(bm, tx, ty, timp)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Mean loss: 9.875212, Median loss: 6.2169
Best: tensor([ -62.,  104.,  -41.,  141.,  106.,  111.,  126.,   40.,  134.,  -91.,
         137.,  -17.,  -39.,   88.,  -45.,  -23.,  -25., -152.,   91.,   31.]) 
Worst: tensor([ -67.,  114.,  -35.,  234.,  103., -162., -233.,  186.,  -55.,  111.,
         108., -114.,  -50.,  198.,   31., -168.,  119.,    8., -118.,  185.])
tensor([[182, 249,   1],
        [144,  30,  19],
        [202, 237,   2],
        [238,   4,  21],
        [119,  16,   5],
        [ 92, 254,   3],
        [  5, 238, 236],
        [219,  33,  16],
        [ 50, 105, 255],
        [164,  53,  49],
        [151,  43,  37],
        [ 54, 168, 250],
        [ 24,  74, 254],
        [233,  35,  23],
        [247, 216, 243],
        [ 24, 192, 252],
        [143,  24,  20],
        [  8,   0, 248],
        [ 12, 130, 255],
        [234,  49,  45]], device='cuda:0')
#+end_example
[[./.ob-jupyter/b9c3dfab05fa52f1d0dd2a0f2e175f1f6d52ee25.png]]
[[./.ob-jupyter/872c88deef784cceb148e33e7b03f9bc8a49dffe.png]]
:END:

Error analysis:
The MSE is still very high on this, possibly because we overfit, having lowered
the dropout.
let's see at which values of lags the model struggles most:
#+begin_src python
e = (tp - ty.cpu()).square().sum(1)
sortidx = e.argsort()
print("Best:\n",ty.cpu()[sortidx][:10].T, "\nWorst:\n", ty.cpu()[sortidx][-10:].T)
#+end_src

#+RESULTS:
: Best:
:  tensor([[ -55., -136.,  -55.,  119., -185.,   88., -182.,  206.,  104., -106.],
:         [ 105.,  115.,  -46., -141.,   88., -140.,  122., -101., -169.,   58.]]) 
: Worst:
:  tensor([[ 254.,  244.,  246.,    5.,  -89.,  240.,   29.,  -76., -187.,  -45.],
:         [ -76.,  -31.,  -53.,    0.,  166.,  -16.,  158.,  201.,  251.,  233.]])

There are somewhat more extreme values at the large errors, but in general I
think it's just overfit.

** Non-binary impulses
This is a contrived case where we learn impulses, but in reality we'll never
have such data. Let's transform these into gaussian impulses for a further
step, and check whether it still works as well.

#+begin_src python
def transform_impulse1(x, n=11, ramp_up: int = 0):
    c = x.shape[1]
    ls = torch.linspace(-3 * np.e, 0, n, device=x.device)
    exp = torch.exp(ls)
    if ramp_up > 0:
        exp[-ramp_up:] = torch.exp(
            torch.linspace(ls[-ramp_up], 2 * -np.e, ramp_up, device=x.device)
        )
    return F.conv1d(F.pad(x, (n - 1, 0)), exp.repeat(c, 1, 1), groups=c)
#+end_src

#+begin_src python
w = 256
c = 3
lossfun = F.mse_loss
lr = 0.001 * (10 if lossfun == F.mse_loss else 1)
num_epochs = 3000
print_every = 100

m = model.CNN(
    w, c-1, c, layer_sizes=[8, 16, 32, 16, 8], kernel_size=3, dilation=1
).to(device)
m = model.CNNRNN(
    w,
    c-1,
    c,
    layer_sizes=[8],
    kernel_size=2,
    n_hidden=128,
    n_rnn_layers=1,
    dropout_rate=0.6,
).to(device)
#m = model.RNN(w, c - 1, c, 64, 2, dropout_rate=0.5).to(device)
x, y, imp = generate_data(w, c, 100, device=device)
x = transform_impulse1(x, 200, 20)
tx, ty, timp = generate_data(w, c, 1000, device=device)
tx = transform_impulse1(tx, 200, 20)

optimizer = optim.NAdam(m.parameters(), lr=lr, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)

errors, bm = train(
    x, y, m, lossfun, optimizer, scheduler, 3000, tx[:100], ty[:100], 500
)
#+end_src

#+RESULTS:
#+begin_example
/home/tim/projects/loopmate/venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
Epoch 0, TL 10484.0703125 VL: 10384.6708984375
Epoch 100, TL 938.7058715820312 VL: 1289.9053955078125
Epoch 200, TL 541.3031005859375 VL: 1007.843505859375
Epoch 300, TL 518.7640991210938 VL: 1100.35302734375
Epoch 400, TL 894.5806274414062 VL: 573.8170166015625
Epoch 500, TL 1772.584228515625 VL: 913.5153198242188
Epoch 600, TL 429.4293212890625 VL: 925.192626953125
Epoch 700, TL 237.65968322753906 VL: 594.9714965820312
Epoch 800, TL 224.59751892089844 VL: 741.8694458007812
Epoch 900, TL 199.71498107910156 VL: 616.1124877929688
Epoch 1000, TL 227.0703887939453 VL: 622.5178833007812
Epoch 1100, TL 238.2711181640625 VL: 563.7627563476562
Epoch 1200, TL 295.89019775390625 VL: 525.790283203125
Epoch 1300, TL 159.72988891601562 VL: 577.2315063476562
Epoch 1400, TL 194.85133361816406 VL: 517.0321044921875
Epoch 1500, TL 162.3317108154297 VL: 512.5814819335938
Epoch 1600, TL 147.8549346923828 VL: 580.2058715820312
Epoch 1700, TL 184.61245727539062 VL: 494.27886962890625
Epoch 1800, TL 135.51132202148438 VL: 535.162353515625
Epoch 1900, TL 99.11566162109375 VL: 567.2203369140625
Epoch 2000, TL 136.7459716796875 VL: 691.7279052734375
Epoch 2100, TL 159.35519409179688 VL: 484.2505187988281
Stopping at epoch 2142!
Epoch 2142, Loss 169.32957458496094
tensor([[  44.7613,  -16.7507],
        [ -45.2762,   46.2348],
        [ -71.9179,   69.3863],
        [  85.7112,  -42.3084],
        [  83.9954,   -7.8040],
        [-188.9619,  162.8197],
        [ -32.8533,   18.9825],
        [ -83.5417,   34.0097],
        [  43.6509,   93.1765],
        [ -10.9903,  -91.1194]], grad_fn=<SliceBackward0>) 
 tensor([[  55.,  -15.],
        [ -43.,   43.],
        [ -99.,   89.],
        [  97.,  -40.],
        [  87.,   -9.],
        [-163.,  141.],
        [ -27.,   15.],
        [-100.,   45.],
        [  42.,   78.],
        [  -5.,  -94.]])
#+end_example

#+begin_src python :async no
error_analysis(bm, tx, ty, timp)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Mean loss: 16.629585, Median loss: 12.7509
Best: tensor([  53.,  -45.,   51.,   95., -116.,    4.,  -22.,   58.,   83.,   21.,
         149.,  -97.,  -77.,  148.,  -70.,  100.,  -41.,  135.,   12.,  194.]) 
Worst: tensor([-175.,   26.,  -53.,  240.,  -76.,   59.,   23.,  -49.,   35.,  191.,
          15., -100.,  -36.,   89.,   40.,   29.,   20., -246.,   -2.,   50.])
tensor([[  4, 179,  52],
        [ 43,  17,  31],
        [121, 174, 146],
        [244,   4,  55],
        [175, 251, 142],
        [ 64,   5,  46],
        [ 69,  46,  39],
        [ 31,  80,  48],
        [ 61,  26, 250],
        [192,   1, 200],
        [ 53,  38,  30],
        [151, 251, 124],
        [ 78, 114,  93],
        [110,  21, 131],
        [ 45,   5,  28],
        [ 30,   1, 173],
        [ 20,   0, 221],
        [  1, 247,   9],
        [  1,   3,  40],
        [ 51,   1,  29]])
#+end_example
[[./.ob-jupyter/4b24b106db1f60b818317e1012fd3d1d59bfdf8a.png]]
[[./.ob-jupyter/729e0dddeb7fd93a9fad8a31c3799117a1f60fbd.png]]
:END:

Nice, it performs pretty much the same!

*** Additional changes
This is still very idealized - here are more things we can do to make it look
more real:
- peaks at different amplitudes
- modulate with sine wave
- add noise


Note: frequencies should be the same in each of the channels, phase could be
slightly shifted, but very little. The sine needs to start at the impulse in
each case, so currently this is wrong.
#+begin_src python
def transform_impulse2(
    x, imp, random_phase: bool = False, noise_std=0, sr=96000
):
    n, c, w = x.shape
    ls = torch.linspace(0, x.shape[-1] / sr, x.shape[-1], device=x.device)
    phase = (
        torch.rand(x.shape[0], x.shape[1], 1) * 0.1 * np.pi
        if random_phase
        else 0
    )
    f = torch.randint(300, 1000, (x.shape[0], 1, 1)).expand(n, c, 1)
    sin = torch.sin(2 * np.pi * ls[None, None, :] * f + phase)
    for i in range(len(x)):
        for j in range(c):
            k = w - imp[i, j]
            x[i, j, imp[i, j] :] *= sin[i, j, :k]
    x += torch.randn(x.shape) * noise_std
    return x
#+end_src

#+begin_src python
x = transform_impulse2(x, imp, True, 0.001)
tx = transform_impulse2(tx, timp, True, 0.001)

optimizer = optim.NAdam(m.parameters(), lr=lr, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)

errors, bm = train(
    x, y, m, lossfun, optimizer, scheduler, 3000, tx[:100], ty[:100], 500
)
#+end_src

#+RESULTS:
#+begin_example
Epoch 0, TL 376.1084899902344 VL: 1394.9144287109375
Epoch 100, TL 213.19107055664062 VL: 852.169189453125
Epoch 200, TL 186.03887939453125 VL: 1102.3375244140625
Epoch 300, TL 234.40835571289062 VL: 1363.6737060546875
Epoch 400, TL 222.33616638183594 VL: 994.2564697265625
Epoch 500, TL 293.6726379394531 VL: 953.693359375
Epoch 600, TL 157.6733856201172 VL: 910.337646484375
Stopping at epoch 635!
Epoch 635, Loss 179.98292541503906
tensor([[  65.6227,   -4.7834],
        [ -38.2449,   49.4815],
        [ -99.8921,   87.3010],
        [ 104.6357,  -36.9680],
        [  94.0138,    9.1724],
        [-158.8137,  147.9350],
        [ -19.0425,   15.0583],
        [ -95.2740,   48.8997],
        [  42.6091,   86.5291],
        [ -10.2183, -103.7110]], grad_fn=<SliceBackward0>) 
 tensor([[  55.,  -15.],
        [ -43.,   43.],
        [ -99.,   89.],
        [  97.,  -40.],
        [  87.,   -9.],
        [-163.,  141.],
        [ -27.,   15.],
        [-100.,   45.],
        [  42.,   78.],
        [  -5.,  -94.]])
#+end_example

#+begin_src python :async no
error_analysis(bm, tx, ty, timp)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Mean loss: 26.891285, Median loss: 21.2385
Best: tensor([ 121.,   82.,    6.,   56.,   57.,  -39.,  -76.,  -40.,  -33.,   11.,
          48.,   34.,  163.,   13.,   31.,  140., -195.,  104.,   78., -177.]) 
Worst: tensor([ -14., -174.,  -51.,  -79.,  -31.,   89., -212., -170., -114., -148.,
        -156., -196.,  -40., -126., -170., -207., -162., -174., -152., -200.])
tensor([[ 51,  65,  12],
        [  1, 175, 143],
        [204, 255,  54],
        [ 85, 164,  63],
        [ 39,  70,   9],
        [ 94,   5, 123],
        [ 15, 227, 153],
        [ 77, 247,  70],
        [ 87, 201,  41],
        [ 96, 244, 105],
        [ 90, 246, 107],
        [  5, 201, 123],
        [ 37,  77,   1],
        [ 11, 137,  10],
        [ 77, 247,  27],
        [  6, 213, 234],
        [  2, 164, 254],
        [ 23, 197, 104],
        [100, 252,  16],
        [ 54, 254,  26]])
#+end_example
[[./.ob-jupyter/446d89edd4d28360d8be779f30557a7fc572f0fe.png]]
[[./.ob-jupyter/b2f3d8a75d7a1341571bf3a6047131f020001698.png]]
:END:

#+begin_src python
m = model.CNN(
    w, c - 1, c, layer_sizes=[8, 16, 32, 16, 8], kernel_size=3, dilation=1
).to(device)
m = model.CNNRNN(
    w,
    c-1,
    c,
    layer_sizes=[8],
    kernel_size=2,
    n_hidden=128,
    n_rnn_layers=1,
    dropout_rate=0.6,
).to(device)

# m = model.RNN(w, c - 1, c, 64, 2, dropout_rate=0.5).to(device)
x, y, imp = generate_data(w, c, 100, device=device)
x = transform_impulse1(x, 200, 20)
x = transform_impulse2(x, imp, True, 0.001)
tx, ty, timp = generate_data(w, c, 1000, device=device)
tx = transform_impulse1(tx, 200, 20)
tx = transform_impulse2(tx, timp, True, 0.001)

optimizer = optim.NAdam(m.parameters(), lr=lr, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)

errors, bm = train(
    x, y, m, lossfun, optimizer, scheduler, 3000, tx[:100], ty[:100], 500
)
#+end_src

#+RESULTS:
#+begin_example
/home/tim/projects/loopmate/venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
Epoch 0, TL 11822.8984375 VL: 9356.166015625
Epoch 100, TL 3676.6650390625 VL: 2760.324462890625
Epoch 200, TL 1373.1256103515625 VL: 1626.67333984375
Epoch 300, TL 807.0391235351562 VL: 2814.339599609375
Epoch 400, TL 446.0246887207031 VL: 1248.6492919921875
Epoch 500, TL 2555.778076171875 VL: 2435.467041015625
Epoch 600, TL 258.6869812011719 VL: 1546.9642333984375
Epoch 700, TL 294.29229736328125 VL: 1524.66748046875
Stopping at epoch 770!
Epoch 770, Loss 213.75682067871094
tensor([[  -4.2411,   42.5790],
        [  49.6260, -182.2782],
        [-126.0361,  111.3190],
        [ -29.3254,   23.7332],
        [ 130.6279,  -26.8274],
        [ -77.3261,  -22.0663],
        [ -56.0428,   27.7260],
        [ 176.5983,  -28.5900],
        [  38.8783,   15.8158],
        [ -44.8339, -117.8624]], grad_fn=<SliceBackward0>) 
 tensor([[   0.,   62.],
        [  30., -178.],
        [-129.,  129.],
        [ -38.,   23.],
        [ 125.,  -22.],
        [ -82.,  -27.],
        [ -64.,   47.],
        [ 187.,  -26.],
        [  46.,   19.],
        [ -49., -114.]])
#+end_example


#+begin_src python :async no
error_analysis(bm, tx, ty, timp)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Mean loss: 41.891613, Median loss: 34.3990
Best: tensor([  38.,   26.,   74.,  -30.,  186.,  -48.,  -10., -123., -133.,  -18.,
           0.,  109.,   -3., -236.,   76.,   51., -201.,   50.,   21.,  -98.]) 
Worst: tensor([ -65.,  -76.,  -45.,  145.,   -5., -211.,   30., -168.,  -16., -211.,
         -30.,   68.,  159., -227.,  119.,  140.,  -33.,   11.,  -16.,  -63.])
tensor([[ 27,  92, 236],
        [ 28, 104, 127],
        [ 38,  83, 112],
        [224,  79, 254],
        [  1,   6,  72],
        [ 29, 240,  12],
        [ 31,   1,  54],
        [  0, 168,  65],
        [ 19,  35, 101],
        [ 41, 252,  13],
        [  0,  30, 106],
        [158,  90, 240],
        [174,  15, 252],
        [  4, 231,  39],
        [200,  81, 245],
        [193,  53, 242],
        [ 11,  44, 126],
        [ 29,  18, 125],
        [ 15,  31, 132],
        [  2,  65, 249]])
#+end_example
[[./.ob-jupyter/5e54fec1e73de8acfa75c583e496725a286a0f9c.png]]
[[./.ob-jupyter/15e13324b7d0432cf8b0bdf077aa7192c61a2696.png]]
:END:


*** Making the data even more real

In its current iteration, the data models an impulse of the fundamental - but
as far as the modelling problem goes, it's different from what we'll see in
realtime: There, we'll always start the window from the first onset on. In the
current data, the first onset may start anywhere.

Let's adapt the data in such a way that our first onset is always close to the
beginning of the buffers.
#+begin_src python
def generate_data2(w: int, c: int, n: int = 10000, max_shift=10, device=None):
    signals = torch.zeros(n, c, w, device=device)
    impulses = torch.randint(0, w - max_shift, (n, c), device=device)
    mini = impulses.min(dim=1, keepdim=True).values
    impulses -= mini
    impulses += torch.maximum(
        torch.tensor(0),
        torch.minimum(
            w - impulses.max(dim=1, keepdim=True).values - 1,
            torch.randint(max_shift, (len(impulses), 1)),
        ),
    )
    signals.scatter_(2, impulses[:, :, None], 1)
    lags = []
    for i in range(c - 1):
        lags.append(impulses[:, i] - impulses[:, i + 1])
    return signals, torch.stack(lags, -1).to(torch.float32), impulses
#+end_src


#+begin_src python
w = 256
c = 2
lossfun = F.mse_loss
lr = 0.001 * (10 if lossfun == F.mse_loss else 1)
num_epochs = 2000
print_every = 100

m = model.CNN(w, c-1, c, layer_sizes=[8,8], kernel_size=8, dropout_rate=0.9).to(
    device
)
#m = model.RNN(w, c-1, c, 32, 1, dropout_rate=0.6, rnn_type="GRU").to(device)
# m = model.CNNRNN(
#     w,
#     c - 1,
#     c,
#     layer_sizes=[9, 18, 27],
#     kernel_size=3,
#     n_hidden=64,
#     n_rnn_layers=2,
#     dropout_rate=0.8,
#     groups=1,
# ).to(device)
x, y, imp = generate_data2(w, c, 100, 100, device=device)
x = transform_impulse1(x, 200, 20)
x = transform_impulse2(x, imp, True, 0.01)
tx, ty, timp = generate_data2(w, c, 1000, 100, device=device)
tx = transform_impulse1(tx, 200, 20)
tx = transform_impulse2(tx, timp, True, 0.01)

optimizer = optim.NAdam(m.parameters(), lr=lr, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs/10)

errors, bm = train(
    x, y, m, lossfun, optimizer, scheduler, 3000, tx[:100], ty[:100], 500
)
#+end_src

#+RESULTS:
#+begin_example
Epoch 0, TL 4036.797607421875 VL: 3480.2412109375
Epoch 100, TL 854.27734375 VL: 765.409423828125
Epoch 200, TL 650.5879516601562 VL: 641.4112548828125
Epoch 300, TL 630.3883666992188 VL: 347.8817138671875
Epoch 400, TL 639.2597045898438 VL: 401.2127990722656
Epoch 500, TL 371.78668212890625 VL: 363.1514587402344
Epoch 600, TL 327.7192077636719 VL: 317.89971923828125
Epoch 700, TL 419.5483093261719 VL: 273.0123596191406
Epoch 800, TL 420.24371337890625 VL: 267.8271179199219
Epoch 900, TL 391.54119873046875 VL: 206.84585571289062
Epoch 1000, TL 366.1349182128906 VL: 231.00009155273438
Epoch 1100, TL 270.36846923828125 VL: 250.03103637695312
Epoch 1200, TL 257.08135986328125 VL: 168.81224060058594
Epoch 1300, TL 205.8913116455078 VL: 182.5635223388672
Epoch 1400, TL 212.9664306640625 VL: 249.74810791015625
Epoch 1500, TL 221.5785369873047 VL: 202.1371307373047
Epoch 1600, TL 199.90847778320312 VL: 229.76846313476562
Epoch 1700, TL 293.56805419921875 VL: 220.6162109375
Epoch 1800, TL 253.05361938476562 VL: 147.6875457763672
Epoch 1900, TL 220.58563232421875 VL: 190.52951049804688
Stopping at epoch 1996!
Epoch 1996, Loss 291.4825134277344
tensor([[-119.0528],
        [ -42.7661],
        [  97.4705],
        [  -8.6875],
        [  25.9479],
        [  46.6725],
        [ -29.1970],
        [ -33.9167],
        [  22.7498],
        [  67.6298]], grad_fn=<SliceBackward0>) 
 tensor([[-105.],
        [ -40.],
        [  66.],
        [ -22.],
        [  21.],
        [  20.],
        [ -24.],
        [ -37.],
        [   5.],
        [  76.]])
#+end_example

#+begin_src python :async no
error_analysis(bm, tx, ty, timp)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Mean loss: 12.298682, Median loss: 8.7085
Best: tensor([ 29.,   6.,  32.,  41.,  -7., -10.,  25., -24.,  -7., -74.,  15.,  53.,
         -1., -90.,  64.,  66.,  61.,  -6., -79.,  44.]) 
Worst: tensor([ -83.,  -91.,  -95., -140., -122., -135.,  -76., -108., -121., -119.,
        -128.,  107., -148.,  120., -115.,  142., -127.,  145.,  128., -134.])
tensor([[  6,  89],
        [ 74, 165],
        [ 72, 167],
        [ 11, 151],
        [ 97, 219],
        [ 53, 188],
        [ 98, 174],
        [ 61, 169],
        [ 11, 132],
        [ 90, 209],
        [ 96, 224],
        [206,  99],
        [ 80, 228],
        [215,  95],
        [ 76, 191],
        [221,  79],
        [ 99, 226],
        [223,  78],
        [218,  90],
        [ 99, 233]])
#+end_example
[[./.ob-jupyter/526b048f7651d0e3d3bcf7614de77de7195bf2d9.png]]
[[./.ob-jupyter/3698ba6807602343928ece94f662fc42ac779080.png]]
:END:


* Pre-training
Start with impulse data, and epoch-by-epoch morph it into something looking
more like a real signal.



* Idea
Random tone generator based on FM synthesis or just adding different modulated
sines with a huge space. Then feedback the system by saying like/dislike on
single tones to find a space of settings which are pleasing to the ear.
